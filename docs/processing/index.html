<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>tsflex.processing API documentation</title>
<meta name="description" content="Processing module â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/foundation.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em;padding-left:1em;padding-right:1em}button{display:none}#sidebar{padding:3px;max-width:20em;overflow:hidden;min-width:19.8em}#sidebar > *:last-child{margin-bottom:1cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;border-top:1px solid #ddd;text-align:right}#footer p{}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f1f3f9;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:0.5em;padding:0px}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;max_width:100%;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#ebf3ff}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#edfcf4}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#ffddcc}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:850px){.sidebar_container{display:flex;transition:0.75s ease}.sidebar_small{width:0;margin:0;padding:0}.hide_content{display:none}button{display:initial;float:left;position:sticky;border:none;height:5ch;width:5ch;border-radius:50%;box-shadow:0px 1px 4px 1px rgba(0,0,0,.2);top:5%;left:100%;transform:translateX(-50%);cursor:pointer}#sidebar{width:25%;height:100vh;overflow:auto;position:sticky;top:0;transition:0.75s ease}#index_button_img{opacity:0.65}#content{max-width:105ch;padding:2em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1em;padding-right:0.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-212611910-1"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-212611910-1');
</script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="https://media.discordapp.net/attachments/372491075153166338/852906324417445908/icon.png">
</head>
<body>
<main>
<article id="content">
<button id="index_button_button"><img id="index_button_img"
src="https://image.flaticon.com/icons/png/512/56/56763.png"
alt="" width="33" height="25"></button>
<header>
<h1 class="title">Module <code>tsflex.processing</code></h1>
</header>
<section id="section-intro">
<p>Processing module.</p>
<h1 id="processing-guide">Processing guide</h1>
<p>The following sections will explain the processing module in detail.</p>
<!-- <div style="text-align: center;"> -->
<h3><b><a href="#header-submodules">Jump to API reference</a></b></h3>
<!-- </div> -->
<p><br></p>
<h2 id="working-example">Working example âœ…</h2>
<p><em>tsflex</em> is built to be intuitive, so we encourage you to copy-paste this code and toy with some parameters! <br></p>
<p>This executable example creates a processing pipeline that contains 3 processing steps (abs, median filter, and detreding, each applied on a different subset of series). <br></p>
<pre><code class="language-python">import pandas as pd; import scipy.signal as ss; import numpy as np
from tsflex.processing import SeriesProcessor, SeriesPipeline

# 1. -------- Get your time-indexed data --------
# Data contains 3 columns; [&quot;ACC_x&quot;, &quot;ACC_y&quot;, &quot;ACC_z&quot;]
url = &quot;https://github.com/predict-idlab/tsflex/raw/main/examples/data/empatica/&quot;
data = pd.read_parquet(url + &quot;acc.parquet&quot;).set_index(&quot;timestamp&quot;)

# 2 -------- Construct your processing pipeline --------
processing_pipe = SeriesPipeline(
    processors=[
        SeriesProcessor(function=np.abs, series_names=[&quot;ACC_x&quot;, &quot;ACC_y&quot;]),
        SeriesProcessor(ss.medfilt, [&quot;ACC_y&quot;, &quot;ACC_z&quot;], kernel_size=5) 
    ]
)
# -- 2.1. Append processing steps to your processing pipeline
processing_pipe.append(SeriesProcessor(ss.detrend, [&quot;ACC_x&quot;, &quot;ACC_z&quot;]))

# 3 -------- Process the data --------
processing_pipe.process(data=data, return_df=True)
# which outputs:
</code></pre>
<table>
<thead>
<tr>
<th align="left">timestamp</th>
<th align="right">ACC_x</th>
<th align="right">ACC_y</th>
<th align="right">ACC_z</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">2017-06-13 14:22:13+02:00</td>
<td align="right">-32.8736</td>
<td align="right">5.0000</td>
<td align="right">51.1051</td>
</tr>
<tr>
<td align="left">2017-06-13 14:22:13.031250+02:00</td>
<td align="right">-32.8737</td>
<td align="right">5.0000</td>
<td align="right">51.1051</td>
</tr>
<tr>
<td align="left">2017-06-13 14:22:13.062500+02:00</td>
<td align="right">-32.8738</td>
<td align="right">5.0000</td>
<td align="right">51.105</td>
</tr>
<tr>
<td align="left">2017-06-13 14:22:13.093750+02:00</td>
<td align="right">-32.8739</td>
<td align="right">5.0000</td>
<td align="right">51.105</td>
</tr>
<tr>
<td align="left">2017-06-13 14:22:13.125000+02:00</td>
<td align="right">-32.8740</td>
<td align="right">5.0000</td>
<td align="right">51.1049</td>
</tr>
<tr>
<td align="left">&hellip;</td>
<td align="right">&hellip;</td>
<td align="right">&hellip;</td>
<td align="right">&hellip;</td>
</tr>
<tr>
<td align="left"><br></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>More advanced processing examples can be found <a href="https://github.com/predict-idlab/tsflex/tree/main/examples">in these example notebooks</a></p>
</div>
<p><br></p>
<h2 id="getting-started">Getting started ðŸš€</h2>
<p>The processing functionality of <em>tsflex</em> is provided by a <code><a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a></code> that contains <code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code> steps. The processing steps are applied sequentially on the data that is passed to the processing pipeline.</p>
<h3 id="components">Components</h3>
<p><img alt="processing uml" src="https://raw.githubusercontent.com/predict-idlab/tsflex/main/docs/_static/series_uml.png"></p>
<p>As shown above, there are 2 relevant classes for processing.</p>
<ol>
<li><a href="/tsflex/processing/#tsflex.processing.SeriesPipeline">SeriesPipeline</a>: serves as a pipeline, withholding the to-be-applied <em>processing steps</em></li>
<li><a href="/tsflex/processing/#tsflex.processing.SeriesProcessor">SeriesProcessor</a>: an instance of this class describes a <em>processing step</em>. <br>Processors are defined by:<ul>
<li><code>function</code>: the <em>Callable</em> processing-function - e.g. <em>scipy.signal.detrend</em></li>
<li><code>series_names</code>: the <em>name(s)</em> of the series on which the processing function should be applied</li>
<li><code>**kwargs</code>: the <em>keyword arguments</em> for the <code>function</code>.</li>
</ul>
</li>
</ol>
<p>The snippet below shows how the <code><a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a></code> &amp; <code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code> components work:</p>
<pre><code class="language-python">import numpy as np; import scipy.signal as ss
from tsflex.processing import SeriesProcessor, SeriesPipeline

# The SeriesPipeline takes a List[SeriesProcessor] as input
processing_pipe = SeriesPipeline(processors=[
        SeriesProcessor(np.abs, [&quot;series_a&quot;, &quot;series_b&quot;]),
        SeriesProcessor(ss.medfilt, &quot;series_b&quot;, kernel_size=5) # (with kwargs)
    ]
)
# We can still append processing steps after instantiating.
processing_pipe.append(processor=SeriesProcessor(ss.detrend, &quot;series_a&quot;))

# Apply the processing steps
processing_pipe.process(...)
</code></pre>
<h3 id="processing-functions">Processing functions</h3>
<p>The function that processes the series should match this prototype:</p>
<pre><code>function(*series: pd.Series, **kwargs)
    -&gt; Union[np.ndarray, pd.Series, pd.DataFrame, List[pd.Series]]
</code></pre>
<p>Hence, the processing function should take one (or multiple) series as input, these may be followed by some keyword arguments. The output of a processing function can be rather versatile.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A function that processes a <code>np.ndarray</code> instead of <code>pd.Series</code>
should work just fine.</p>
</div>
<p>In <a href="#advanced-usage">this section</a> you can find more info on advanced usage of processing functions.</p>
<p><br></p>
<h2 id="important-notes">Important notes ðŸ“¢</h2>
<!-- As processing steps (<code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code>s) are applied sequentially in the processing pipeline (<code><a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a></code>), the order of these steps (might) affect the output. -->
<p>In a <code><a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a></code> it is common behavior that series are <strong>transformed</strong> (i.e., replaced).
Hence, it is important to keep the following principles in mind when:</p>
<ul>
<li>Applying processing functions that take 1 series as input will (generally) <b style="color:red">transform (i.e., replace) the input series</b> .<ul>
<li><strong>Countermeassure</strong>: this behavior does not occur when the processing function returns a <code>pd.Series</code> <em>with a different name</em>.</li>
</ul>
</li>
<li>The order of steps (<code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code>s) in the processing pipeline (<code><a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a></code>) might affect the output.</li>
</ul>
<p><br></p>
<h2 id="advanced-usage">Advanced usage ðŸ‘€</h2>
<h3 id="versatile-processing-functions">Versatile processing functions</h3>
<p>As <a href="#processing-functions">explained above</a> <em>tsflex</em> is rather versatile in terms of function input and output.</p>
<p><em>tsflex</em> does not just allow <code>one-to-one</code> processing functions, but also <code>many-to-one</code>, <code>one-to-many</code>, and <code>many-to-many</code> functions are supported in a convenient way:</p>
<ul>
<li>
<p><code>many-to-one</code>; the <strong>processing function</strong> should </p>
<ul>
<li>take multiple series as input </li>
<li>output a single array or (named!) series / dataframe with 1 column</li>
</ul>
<p>Example</p>
</li>
</ul>
<pre><code class="language-python">def abs_diff(s1: pd.Series, s2: pd.Series) -&gt; pd.Series:
    return pd.Series(np.abs(s1-s2), name=f&quot;abs_diff_{s1.name}-{s2.name}&quot;)
</code></pre>
<ul>
<li>
<p><code>one-to-many</code>; the <strong>processing function</strong> should</p>
<ul>
<li>take a single series as input
</li>
<li>output a list of (named!) series or a dataframe with multiple columns</li>
</ul>
<p>Example</p>
</li>
</ul>
<pre><code class="language-python">def abs_square(s1: pd.Series) -&gt; List[pd.Series]:
    s1_abs = pd.Series(np.abs(s1), name=f&quot;abs_{s1.name}&quot;)
    s1_square = pd.Series(np.square(s1), name=f&quot;square_{s1.name}&quot;)
    return [s1_abs, s1_square]
</code></pre>
<ul>
<li>
<p><code>many-to-many</code>; <em>(combination of the above)</em> the <strong>processing function</strong> should</p>
<ul>
<li>take multiple series as input</li>
<li>output a list of (named!) series or a dataframe with multiple columns</li>
</ul>
<p>Example</p>
</li>
</ul>
<pre><code class="language-python">def abs_square_diff(s1: pd.Series, s2: pd.Series) -&gt; List[pd.Series]:
    s_abs = pd.Series(np.abs(s1-s2), name=f&quot;abs_{s1.name}-{s2.name}&quot;)
    s_square = pd.Series(np.square(s1-s2), name=f&quot;square_{s1.name}-{s2.name}&quot;)
    return [s_abs, s_square]
</code></pre>
<h3 id="dataframe-decorator">DataFrame decorator</h3>
<p>In some (rare) cases a processing function requires a <code>pd.DataFrame</code> as input.
For these cases we provide the <a href="#tsflex.processing.dataframe_func">dataframe_func decorator</a>. This decorator wraps the processing function in the <code><a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a></code>, provided a <code>pd.DataFrame</code> as input instead of multiple <code>pd.Series</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In most cases series arguments are sufficient; you can perform column-based operations on multiple <code>pd.Series</code> (e.g., subtract 2 series). Only when row-based operations are required (e.g., <code>df.dropna(axis=0)</code>), a <code>pd.DataFrame</code> is unavoidable.</p>
</div>
<h3 id="logging">Logging</h3>
<p>When a <code>logging_file_path</code> is passed to the <code><a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a></code> its <code>process</code> method, the execution times of the processing steps will be logged.</p>
<p><a href="#tsflex.processing.get_processor_logs">More info</a></p>
<p><br></p>
<hr>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Processing module.

.. include:: ../../docs/pdoc_include/processing.md

&#34;&#34;&#34;

__author__ = &#34;Jonas Van Der Donckt, Emiel Deprost, Jeroen Van Der Donckt&#34;

from .logger import get_processor_logs
from .series_pipeline import SeriesPipeline
from .series_processor import SeriesProcessor, dataframe_func
from .. import __pdoc__

__pdoc__[&#39;SeriesProcessor.__call__&#39;] = True

__all__ = [
    &#34;dataframe_func&#34;,
    &#34;SeriesProcessor&#34;,
    &#34;SeriesPipeline&#34;,
    &#34;get_processor_logs&#34;,
]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">API reference of <code>tsflex.processing</code></h2>
<dl>
<dt><code class="name"><a title="tsflex.processing.logger" href="logger.html">.logger</a></code></dt>
<dd>
<div class="desc"><p>Contains the used variables and functions to provide logging functionality â€¦</p></div>
</dd>
<dt><code class="name"><a title="tsflex.processing.series_pipeline" href="series_pipeline.html">.series_pipeline</a></code></dt>
<dd>
<div class="desc"><p>SeriesPipeline class for time-series data (pre-)processing pipeline.</p></div>
</dd>
<dt><code class="name"><a title="tsflex.processing.series_processor" href="series_processor.html">.series_processor</a></code></dt>
<dd>
<div class="desc"><p>Code for time-series data (pre-)processing.</p></div>
</dd>
<dt><code class="name"><a title="tsflex.processing.utils" href="utils.html">.utils</a></code></dt>
<dd>
<div class="desc"><p>(Advanced) utilities for the processing pipelines.</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tsflex.processing.dataframe_func"><code class="name flex">
<span>def <span class="ident">dataframe_func</span></span>(<span>func)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dataframe_func(func: Callable):
    &#34;&#34;&#34;Decorate function to use a DataFrame instead of multiple series (as argument).

    This decorator can be used for functions that need to work on a whole
    `pd.DataFrame`. It will convert the required series into a DataFrame using an
    **outer merge**.

    The function&#39;s prototype should be:

        func(df : pd.DataFrame, **kwargs)
            -&gt; Union[np.ndarray, pd.Series, pd.DataFrame, List[pd.Series]]

    So the decorated `func` has to take a DataFrame as first argument.

    Notes
    -----
    Only when you want to perform row-based operations, such as `df.dropna(axis=0)`,
    this wrapper is needed.
    Hence, in most cases that `func` requires a `pd.DataFrame`, series arguments would
    be sufficient; as you can perform column-based operations on multiple `pd.Series`
    (e.g., subtract 2 series) and most dataframe operations are also available for a
    `pd.Series`.

    &#34;&#34;&#34;

    def wrapper(*series: pd.Series, **kwargs):
        series_dict = {s.name: s for s in series}
        df = series_dict_to_df(series_dict)
        res = func(df, **kwargs)
        return res

    wrapper.__name__ = &#34;dataframe_func: &#34; + func.__name__
    return wrapper</code></pre>
</details>
<div class="desc"><p>Decorate function to use a DataFrame instead of multiple series (as argument).</p>
<p>This decorator can be used for functions that need to work on a whole
<code>pd.DataFrame</code>. It will convert the required series into a DataFrame using an
<strong>outer merge</strong>.</p>
<p>The function's prototype should be:</p>
<pre><code>func(df : pd.DataFrame, **kwargs)
    -&gt; Union[np.ndarray, pd.Series, pd.DataFrame, List[pd.Series]]
</code></pre>
<p>So the decorated <code>func</code> has to take a DataFrame as first argument.</p>
<h2 id="notes">Notes</h2>
<p>Only when you want to perform row-based operations, such as <code>df.dropna(axis=0)</code>,
this wrapper is needed.
Hence, in most cases that <code>func</code> requires a <code>pd.DataFrame</code>, series arguments would
be sufficient; as you can perform column-based operations on multiple <code>pd.Series</code>
(e.g., subtract 2 series) and most dataframe operations are also available for a
<code>pd.Series</code>.</p></div>
</dd>
<dt id="tsflex.processing.get_processor_logs"><code class="name flex">
<span>def <span class="ident">get_processor_logs</span></span>(<span>logging_file_path)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_processor_logs(logging_file_path: str) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Get execution (time) info for each processor of a ``SeriesPipeline``.

    Parameters
    ----------
    logging_file_path: str
        The file path where the logged messages are stored. This is the file path that
        is passed to the ``SeriesPipeline`` its ``process`` method.

    Returns
    -------
    pd.DataFrame
        A DataFrame containing each processor its duration and its series names.
    
    &#34;&#34;&#34;
    df = _parse_logging_execution_to_df(logging_file_path)
    df[&#34;duration&#34;] = pd.to_timedelta(df[&#34;duration&#34;], unit=&#34;s&#34;)
    return df</code></pre>
</details>
<div class="desc"><p>Get execution (time) info for each processor of a <code><a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a></code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>logging_file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The file path where the logged messages are stored. This is the file path that
is passed to the <code><a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a></code> its <code>process</code> method.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>A DataFrame containing each processor its duration and its series names.</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tsflex.processing.SeriesProcessor"><code class="flex name class">
<span>class <span class="ident">SeriesProcessor</span></span>
<span>(</span><span>function, series_names, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SeriesProcessor(FrozenClass):
    &#34;&#34;&#34;Class that executes a specific operation on the passed series_dict.

    Parameters
    ----------
    function : Callable
        The function that processes the series (given in the `series_names`).
        The prototype of the function should match: \n

            function(*series: pd.Series, **kwargs)
                -&gt; Union[np.ndarray, pd.Series, pd.DataFrame, List[pd.Series]]

        .. note::
            A function that processes a ``np.ndarray`` instead of ``pd.Series``
            should work just fine.

    series_names : Union[str, Tuple[str, ...], List[str], List[Tuple[str, ...]]]
        The names of the series on which the processing function should be applied.

        This argument should match the `function` its input; \n
        * If `series_names` is a (list of) string (or tuple of a single string),
          than `function` should require just one series as input.
        * If `series_names` is a (list of) tuple of strings, than `function` should
          require `len(tuple)` series as input **and in exactly the same order**.

        A list means multiple series (combinations) to process; \n
        * If `series_names` is a string or a tuple of strings, than `function` will
          be called only once for the series of this argument.
        * If `series_names` is a list of either strings or tuple of strings, than
          `function` will be called for each entry of this list.

        .. note::
            when passing a list as `series_names`, all items in this list should
            have the same type, i.e, either \n
            * all a `str`
            * or, all a `tuple` _with same length_. \n

    **kwargs: dict, optional
        Keyword arguments which will be also passed to the `function`

    Notes
    -----
    If the output of `function` is a `np.ndarray` or a `pd.Series` without a name,
    than (items of) the given `series_names` must have length 1, i.e., the function
    requires just 1 series! That series its name and index are used to transform
    (i.e., **replace**) that series.

    If you want to transform (i.e., **replace**) the input series with the
    processor, than `function` should return either: \n
    * a `np.ndarray` (see above).
    * a `pd.Series` with no name or with the same name as the input series.
    * a `pd.DataFrame` with (one) column name equal to the input series its name.
    * a list of `pd.Series` in which (exact) one series has the same name as the
      input series.

    Series (&amp; columns) with other (column) names will be added to the series dict.

    &#34;&#34;&#34;
    def __init__(
        self,
        function: Callable,
        series_names: Union[str, Tuple[str, ...], List[str], List[Tuple[str, ...]]],
        **kwargs,
    ):
        series_names = [to_tuple(names) for names in to_list(series_names)]
        # Assert that function inputs (series) all have the same length
        assert all(
            len(series_names[0]) == len(series_name_tuple)
            for series_name_tuple in series_names
        )
        self.series_names: List[Tuple[str, ...]] = series_names
        self.function = function
        self.name = self.function.__name__

        self.kwargs = kwargs
        self._freeze()

    def get_required_series(self) -&gt; List[str]:
        &#34;&#34;&#34;Return all required series names for this processor.

        Return the list of series names that are required in order to execute the
        processing function.

        Returns
        -------
        List[str]
            List of all the required series names.

        &#34;&#34;&#34;
        return list(set(flatten(name for name in self.series_names)))

    def __call__(self, series_dict: Dict[str, pd.Series]) -&gt; Dict[str, pd.Series]:
        &#34;&#34;&#34;**Call**culates the processed series.

        Parameters
        ----------
        series_dict : Dict[str, pd.Series]
            A dict of `pd.Series` containing the data that need to be processed.
            The key should always be the accompanying series its name.

        Returns
        -------
        Dict[str, pd.Series]
            The processed `series_dict`.

        Raises
        ------
        KeyError
            Raised when a key is not present in the `series_dict` but required for the
            processing.
        TypeError
            Raised when the output of the `SeriesProcessor` is not of the correct type.

        Notes
        -----
        * The `series_dict` is an internal representation of the time-series data .
          This internal representation is constructed in the `process` method of the
          `SeriesPipeline`.
        * If you want to test or debug your `SeriesProcessor` object, just encapsulate
          your instance of this class in a `SeriesPipeline`. The latter allows more
          versatile input for its `process` method.

        &#34;&#34;&#34;
        t_start = time.time()

        # Only selecting the series that are needed for this processing step
        # requested_dict = {}
        # try:
        #     for sig in self.get_required_series():
        #         requested_dict[sig] = series_dict[sig]
        # except KeyError as key:
        #     # Re raise error as we can&#39;t continue
        #     raise KeyError(
        #         &#34;Key %s is not present in the input dict and needed for processor %s&#34;
        #         % (key, self.name)
        #     )

        # Variable that will contain the final output of this method
        processed_output: Dict[str, pd.Series] = {}

        def get_series_list(keys: Tuple[str, ...]):
            &#34;&#34;&#34;Get an ordered series list view for the given keys.&#34;&#34;&#34;
            return [series_dict[key] for key in keys]

        def get_series_dict(keys: Tuple[str, ...]):
            &#34;&#34;&#34;Get a series dict view for the given keys.&#34;&#34;&#34;
            return {key: series_dict[key] for key in keys}

        for series_name_tuple in self.series_names:
            func_output = self.function(
                *get_series_list(series_name_tuple), **self.kwargs
            )
            func_output = _handle_seriesprocessor_func_output(
                func_output,
                get_series_dict(series_name_tuple),
                self.name,
            )
            # Check that the output of the function call produces unique columns / keys
            assert (
                len(set(processed_output.keys()).intersection(func_output.keys())) == 0
            )
            processed_output.update(func_output)

        elapsed = time.time() - t_start
        logger.info(
            f&#34;Finished function [{self.name}] on {self.series_names} in &#34;
            f&#34;[{elapsed} seconds]!&#34;
        )

        return processed_output

    def __repr__(self):
        &#34;&#34;&#34;Return formal representation of object.&#34;&#34;&#34;
        repr_str = self.name + (&#34; &#34; + str(self.kwargs))
        repr_str += &#34; :  &#34; + &#34; &#34;.join([str(s) for s in self.series_names])
        return repr_str

    def __str__(self):
        &#34;&#34;&#34;Return informal representation of object.&#34;&#34;&#34;
        return self.__repr__()</code></pre>
</details>
<div class="desc"><p>Class that executes a specific operation on the passed series_dict.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>function</code></strong> :&ensp;<code>Callable</code></dt>
<dd>
<p>The function that processes the series (given in the <code>series_names</code>).
The prototype of the function should match: </p>
<pre><code>function(*series: pd.Series, **kwargs)
    -&gt; Union[np.ndarray, pd.Series, pd.DataFrame, List[pd.Series]]
</code></pre>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A function that processes a <code>np.ndarray</code> instead of <code>pd.Series</code>
should work just fine.</p>
</div>
</dd>
<dt><strong><code>series_names</code></strong> :&ensp;<code>Union[str, Tuple[str, &hellip;], List[str], List[Tuple[str, &hellip;]]]</code></dt>
<dd>
<p>The names of the series on which the processing function should be applied.</p>
<p>This argument should match the <code>function</code> its input; </p>
<ul>
<li>If <code>series_names</code> is a (list of) string (or tuple of a single string),
than <code>function</code> should require just one series as input.</li>
<li>If <code>series_names</code> is a (list of) tuple of strings, than <code>function</code> should
require <code>len(tuple)</code> series as input <strong>and in exactly the same order</strong>.</li>
</ul>
<p>A list means multiple series (combinations) to process; </p>
<ul>
<li>If <code>series_names</code> is a string or a tuple of strings, than <code>function</code> will
be called only once for the series of this argument.</li>
<li>If <code>series_names</code> is a list of either strings or tuple of strings, than
<code>function</code> will be called for each entry of this list.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>when passing a list as <code>series_names</code>, all items in this list should
have the same type, i.e, either </p>
<ul>
<li>all a <code>str</code></li>
<li>or, all a <code>tuple</code> <em>with same length</em>.</li>
</ul>
</div>
</dd>
<dt><strong><code>**kwargs</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Keyword arguments which will be also passed to the <code>function</code></dd>
</dl>
<h2 id="notes">Notes</h2>
<p>If the output of <code>function</code> is a <code>np.ndarray</code> or a <code>pd.Series</code> without a name,
than (items of) the given <code>series_names</code> must have length 1, i.e., the function
requires just 1 series! That series its name and index are used to transform
(i.e., <strong>replace</strong>) that series.</p>
<p>If you want to transform (i.e., <strong>replace</strong>) the input series with the
processor, than <code>function</code> should return either: </p>
<ul>
<li>a <code>np.ndarray</code> (see above).</li>
<li>a <code>pd.Series</code> with no name or with the same name as the input series.</li>
<li>a <code>pd.DataFrame</code> with (one) column name equal to the input series its name.</li>
<li>a list of <code>pd.Series</code> in which (exact) one series has the same name as the
input series.</li>
</ul>
<p>Series (&amp; columns) with other (column) names will be added to the series dict.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tsflex.utils.classes.FrozenClass</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="tsflex.processing.SeriesProcessor.get_required_series"><code class="name flex">
<span>def <span class="ident">get_required_series</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_required_series(self) -&gt; List[str]:
    &#34;&#34;&#34;Return all required series names for this processor.

    Return the list of series names that are required in order to execute the
    processing function.

    Returns
    -------
    List[str]
        List of all the required series names.

    &#34;&#34;&#34;
    return list(set(flatten(name for name in self.series_names)))</code></pre>
</details>
<div class="desc"><p>Return all required series names for this processor.</p>
<p>Return the list of series names that are required in order to execute the
processing function.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[str]</code></dt>
<dd>List of all the required series names.</dd>
</dl></div>
</dd>
<dt id="tsflex.processing.SeriesProcessor.__call__"><code class="name flex">
<span>def <span class="ident">__call__</span></span>(<span>self, series_dict)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def __call__(self, series_dict: Dict[str, pd.Series]) -&gt; Dict[str, pd.Series]:
    &#34;&#34;&#34;**Call**culates the processed series.

    Parameters
    ----------
    series_dict : Dict[str, pd.Series]
        A dict of `pd.Series` containing the data that need to be processed.
        The key should always be the accompanying series its name.

    Returns
    -------
    Dict[str, pd.Series]
        The processed `series_dict`.

    Raises
    ------
    KeyError
        Raised when a key is not present in the `series_dict` but required for the
        processing.
    TypeError
        Raised when the output of the `SeriesProcessor` is not of the correct type.

    Notes
    -----
    * The `series_dict` is an internal representation of the time-series data .
      This internal representation is constructed in the `process` method of the
      `SeriesPipeline`.
    * If you want to test or debug your `SeriesProcessor` object, just encapsulate
      your instance of this class in a `SeriesPipeline`. The latter allows more
      versatile input for its `process` method.

    &#34;&#34;&#34;
    t_start = time.time()

    # Only selecting the series that are needed for this processing step
    # requested_dict = {}
    # try:
    #     for sig in self.get_required_series():
    #         requested_dict[sig] = series_dict[sig]
    # except KeyError as key:
    #     # Re raise error as we can&#39;t continue
    #     raise KeyError(
    #         &#34;Key %s is not present in the input dict and needed for processor %s&#34;
    #         % (key, self.name)
    #     )

    # Variable that will contain the final output of this method
    processed_output: Dict[str, pd.Series] = {}

    def get_series_list(keys: Tuple[str, ...]):
        &#34;&#34;&#34;Get an ordered series list view for the given keys.&#34;&#34;&#34;
        return [series_dict[key] for key in keys]

    def get_series_dict(keys: Tuple[str, ...]):
        &#34;&#34;&#34;Get a series dict view for the given keys.&#34;&#34;&#34;
        return {key: series_dict[key] for key in keys}

    for series_name_tuple in self.series_names:
        func_output = self.function(
            *get_series_list(series_name_tuple), **self.kwargs
        )
        func_output = _handle_seriesprocessor_func_output(
            func_output,
            get_series_dict(series_name_tuple),
            self.name,
        )
        # Check that the output of the function call produces unique columns / keys
        assert (
            len(set(processed_output.keys()).intersection(func_output.keys())) == 0
        )
        processed_output.update(func_output)

    elapsed = time.time() - t_start
    logger.info(
        f&#34;Finished function [{self.name}] on {self.series_names} in &#34;
        f&#34;[{elapsed} seconds]!&#34;
    )

    return processed_output</code></pre>
</details>
<div class="desc"><p><strong>Call</strong>culates the processed series.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>series_dict</code></strong> :&ensp;<code>Dict[str, pd.Series]</code></dt>
<dd>A dict of <code>pd.Series</code> containing the data that need to be processed.
The key should always be the accompanying series its name.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, pd.Series]</code></dt>
<dd>The processed <code>series_dict</code>.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>KeyError</code></dt>
<dd>Raised when a key is not present in the <code>series_dict</code> but required for the
processing.</dd>
<dt><code>TypeError</code></dt>
<dd>Raised when the output of the <code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code> is not of the correct type.</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li>The <code>series_dict</code> is an internal representation of the time-series data .
This internal representation is constructed in the <code>process</code> method of the
<code><a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a></code>.</li>
<li>If you want to test or debug your <code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code> object, just encapsulate
your instance of this class in a <code><a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a></code>. The latter allows more
versatile input for its <code>process</code> method.</li>
</ul></div>
</dd>
</dl>
</dd>
<dt id="tsflex.processing.SeriesPipeline"><code class="flex name class">
<span>class <span class="ident">SeriesPipeline</span></span>
<span>(</span><span>processors=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SeriesPipeline:
    &#34;&#34;&#34;Pipeline for applying ``SeriesProcessor`` objects sequentially.

    Parameters
    ----------
    processors : List[Union[SeriesProcessor, SeriesPipeline]], optional
        List of ``SeriesProcessor`` or ``SeriesPipeline`` instances that will be applied
        sequentially to the internal series dict, by default None.
        **The processing steps will be executed in the same order as passed in this
        list.**

    &#34;&#34;&#34;

    def __init__(
        self, processors: Optional[List[Union[SeriesProcessor, SeriesPipeline]]] = None
    ):
        self.processing_steps: List[SeriesProcessor] = []  # TODO: dit private of niet?
        if processors is not None:
            assert isinstance(processors, list)

            self.processing_steps = list(flatten(
                [
                    p.processing_steps if isinstance(p, SeriesPipeline) else [p]
                    for p in processors
                ]
            ))

    def get_required_series(self) -&gt; List[str]:
        &#34;&#34;&#34;Return all required series names for this pipeline.

        Return the list of series names that are required in order to execute all the
        ``SeriesProcessor`` objects of this processing pipeline.

        Returns
        -------
        List[str]
            List of all the required series names.

        &#34;&#34;&#34;
        return list(
            set(flatten(step.get_required_series() for step in self.processing_steps))
        )

    def append(self, processor: SeriesProcessor) -&gt; None:
        &#34;&#34;&#34;Append a ``SeriesProcessor`` at the end of the pipeline.

        Parameters
        ----------
        processor : SeriesProcessor
            The ``SeriesProcessor`` that will be added to the end of the pipeline

        &#34;&#34;&#34;
        self.processing_steps.append(processor)

    def insert(self, idx: int, processor: SeriesProcessor) -&gt; None:
        &#34;&#34;&#34;Insert a ``SeriesProcessor`` at the given index in the pipeline.

        Parameters
        ----------
        idx : int
            The index where the given processor should be inserted in the pipeline.
            Index 0 will insert the given processor at the front of the pipeline,
            and index ``len(pipeline)`` is equivalent to appending the processor.
        processor : SeriesProcessor
            The ``SeriesProcessor`` that will be added to the end of the pipeline

        &#34;&#34;&#34;
        self.processing_steps.insert(idx, processor)

    def process(
        self,
        data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],
        return_df: Optional[bool] = False,
        return_all_series: Optional[bool] = True,
        drop_keys: Optional[List[str]] = None,
        copy: Optional[bool] = False,
        logging_file_path: Optional[Union[str, Path]] = None,
    ) -&gt; Union[List[pd.Series], pd.DataFrame]:
        &#34;&#34;&#34;Execute all ``SeriesProcessor`` objects in pipeline sequentially.

        Apply all the processing steps on passed Series list or DataFrame and return the
        preprocessed Series list or DataFrame.

        Parameters
        ----------
        data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]
            Dataframe or Series or list thereof, with all the required data for the
            processing steps. \n
            **Remark**: each Series / DataFrame must have a ``pd.DatetimeIndex``.
            **Remark**: we assume that each name / column is unique.
        return_df : bool, optional
            Whether the output needs to be a series list or a DataFrame, by default
            False.
            If True the output series will be combined to a DataFrame with an outer
            merge.
        return_all_series : bool, optional
            Whether the output needs to return all the series, by default True.
            * If True the output will contain all series that were passed to this
            method.
            * If False the output will contain just the required series (see
            ``get_required_series``).
        drop_keys : List[str], optional
            Which keys should be dropped when returning the output, by default None.
        copy : bool, optional
            Whether the series in ``data`` should be copied, by default False.
        logging_file_path : Union[str, Path], optional
            The file path where the logged messages are stored, by default None.
            If ``None``, then no logging ``FileHandler`` will be used and the logging
            messages are only pushed to stdout. Otherwise, a logging ``FileHandler`` will
            write the logged messages to the given file path.

        Returns
        -------
        Union[List[pd.Series], pd.DataFrame]
            The preprocessed series.

        Notes
        -----
        * If a ``logging_file_path`` is provided, the execution (time) info can be
          retrieved by calling ``logger.get_processor_logs(logging_file_path)``. &lt;br&gt;
          Be aware that the ``logging_file_path`` gets cleared before the logger pushes
          logged messages. Hence, one should use a separate logging file for each
          constructed processing and feature instance with this library.
        * If a series processor its function output is a ``np.ndarray``, the input series
          dict (required dict for that function) must contain just 1 series! That series
          its name and index are used to return a series dict. When a user does not want
          a numpy array to replace its input series, it is his / her responsibility to
          create a new ``pd.Series`` (or ``pd.DataFrame``) of that numpy array with a
          different (column) name.
        * If ``func_output`` is a ``pd.Series``, keep in mind that the input series gets
          transformed (i.e., replaced) in the pipeline with the ``func_output`` when the
          series name is  equal.

        Raises
        ------
        _ProcessingError
            Error raised when a processing step fails.

        &#34;&#34;&#34;
        # Delete other logging handlers
        delete_logging_handlers(logger)
        # Add logging handler (if path provided)
        if logging_file_path:
            add_logging_handler(logger, logging_file_path)

        # Convert the data to a series_dict
        series_dict: Dict[str, pd.Series] = {}
        for s in to_series_list(data):
            # Assert the assumptions we make!
            if len(s):
                assert isinstance(s.index, pd.DatetimeIndex)
            # TODO: also check monotonic increasing?

            if s.name in self.get_required_series():
                series_dict[str(s.name)] = s.copy() if copy else s
            elif return_all_series:
                # If all the series have to be returned
                series_dict[str(s.name)] = s.copy() if copy else s

        output_keys = set()  # Maintain set of output series
        for processor in self.processing_steps:
            try:
                processed_dict = processor(series_dict)
                output_keys.update(processed_dict.keys())
                series_dict.update(processed_dict)
            except Exception as e:
                raise _ProcessingError(
                    &#34;Error while processing function {}:\n {}&#34;.format(
                        processor.name, str(e)
                    )
                ) from e

        if not return_all_series:
            # Return just the output series
            output_dict = {key: series_dict[str(key)] for key in output_keys}
            series_dict = output_dict

        if drop_keys is not None:
            # Drop the keys that should not be included in the output
            output_dict = {
                key: series_dict[key]
                for key in set(series_dict.keys()).difference(drop_keys)
            }
            series_dict = output_dict

        if return_df:
            # We merge the series dict into a DataFrame
            return series_dict_to_df(series_dict)
        else:
            return [s for s in series_dict.values()]

    def serialize(self, file_path: Union[str, Path]):
        &#34;&#34;&#34;Serialize this ``SeriesPipeline`` instance.

        Notes
        ------
        As we use [Dill](https://github.com/uqfoundation/dill){:target=&#34;_blank&#34;} to
        serialize, we can also serialize (decorator)functions which are defined in the
        local scope, like lambdas.

        Parameters
        ----------
        file_path : Union[str, Path]
            The path where the ``SeriesProcessor`` will be serialized.

        &#34;&#34;&#34;
        with open(file_path, &#34;wb&#34;) as f:
            dill.dump(self, f, recurse=True)

    def __repr__(self):
        &#34;&#34;&#34;Return formal representation of object.&#34;&#34;&#34;
        return &#34;[\n&#34; + &#34;&#34;.join([f&#34;\t{str(p)}\n&#34; for p in self.processing_steps]) + &#34;]&#34;

    def __str__(self):
        &#34;&#34;&#34;Return informal representation of object.&#34;&#34;&#34;
        return self.__repr__()</code></pre>
</details>
<div class="desc"><p>Pipeline for applying <code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code> objects sequentially.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>processors</code></strong> :&ensp;<code>List[Union[<a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a>, <a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a>]]</code>, optional</dt>
<dd>List of <code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code> or <code><a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a></code> instances that will be applied
sequentially to the internal series dict, by default None.
<strong>The processing steps will be executed in the same order as passed in this
list.</strong></dd>
</dl></div>
<h3>Methods</h3>
<dl>
<dt id="tsflex.processing.SeriesPipeline.get_required_series"><code class="name flex">
<span>def <span class="ident">get_required_series</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_required_series(self) -&gt; List[str]:
    &#34;&#34;&#34;Return all required series names for this pipeline.

    Return the list of series names that are required in order to execute all the
    ``SeriesProcessor`` objects of this processing pipeline.

    Returns
    -------
    List[str]
        List of all the required series names.

    &#34;&#34;&#34;
    return list(
        set(flatten(step.get_required_series() for step in self.processing_steps))
    )</code></pre>
</details>
<div class="desc"><p>Return all required series names for this pipeline.</p>
<p>Return the list of series names that are required in order to execute all the
<code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code> objects of this processing pipeline.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[str]</code></dt>
<dd>List of all the required series names.</dd>
</dl></div>
</dd>
<dt id="tsflex.processing.SeriesPipeline.append"><code class="name flex">
<span>def <span class="ident">append</span></span>(<span>self, processor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append(self, processor: SeriesProcessor) -&gt; None:
    &#34;&#34;&#34;Append a ``SeriesProcessor`` at the end of the pipeline.

    Parameters
    ----------
    processor : SeriesProcessor
        The ``SeriesProcessor`` that will be added to the end of the pipeline

    &#34;&#34;&#34;
    self.processing_steps.append(processor)</code></pre>
</details>
<div class="desc"><p>Append a <code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code> at the end of the pipeline.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>processor</code></strong> :&ensp;<code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code></dt>
<dd>The <code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code> that will be added to the end of the pipeline</dd>
</dl></div>
</dd>
<dt id="tsflex.processing.SeriesPipeline.insert"><code class="name flex">
<span>def <span class="ident">insert</span></span>(<span>self, idx, processor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def insert(self, idx: int, processor: SeriesProcessor) -&gt; None:
    &#34;&#34;&#34;Insert a ``SeriesProcessor`` at the given index in the pipeline.

    Parameters
    ----------
    idx : int
        The index where the given processor should be inserted in the pipeline.
        Index 0 will insert the given processor at the front of the pipeline,
        and index ``len(pipeline)`` is equivalent to appending the processor.
    processor : SeriesProcessor
        The ``SeriesProcessor`` that will be added to the end of the pipeline

    &#34;&#34;&#34;
    self.processing_steps.insert(idx, processor)</code></pre>
</details>
<div class="desc"><p>Insert a <code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code> at the given index in the pipeline.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index where the given processor should be inserted in the pipeline.
Index 0 will insert the given processor at the front of the pipeline,
and index <code>len(pipeline)</code> is equivalent to appending the processor.</dd>
<dt><strong><code>processor</code></strong> :&ensp;<code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code></dt>
<dd>The <code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code> that will be added to the end of the pipeline</dd>
</dl></div>
</dd>
<dt id="tsflex.processing.SeriesPipeline.process"><code class="name flex">
<span>def <span class="ident">process</span></span>(<span>self, data, return_df=False, return_all_series=True, drop_keys=None, copy=False, logging_file_path=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process(
    self,
    data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],
    return_df: Optional[bool] = False,
    return_all_series: Optional[bool] = True,
    drop_keys: Optional[List[str]] = None,
    copy: Optional[bool] = False,
    logging_file_path: Optional[Union[str, Path]] = None,
) -&gt; Union[List[pd.Series], pd.DataFrame]:
    &#34;&#34;&#34;Execute all ``SeriesProcessor`` objects in pipeline sequentially.

    Apply all the processing steps on passed Series list or DataFrame and return the
    preprocessed Series list or DataFrame.

    Parameters
    ----------
    data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]
        Dataframe or Series or list thereof, with all the required data for the
        processing steps. \n
        **Remark**: each Series / DataFrame must have a ``pd.DatetimeIndex``.
        **Remark**: we assume that each name / column is unique.
    return_df : bool, optional
        Whether the output needs to be a series list or a DataFrame, by default
        False.
        If True the output series will be combined to a DataFrame with an outer
        merge.
    return_all_series : bool, optional
        Whether the output needs to return all the series, by default True.
        * If True the output will contain all series that were passed to this
        method.
        * If False the output will contain just the required series (see
        ``get_required_series``).
    drop_keys : List[str], optional
        Which keys should be dropped when returning the output, by default None.
    copy : bool, optional
        Whether the series in ``data`` should be copied, by default False.
    logging_file_path : Union[str, Path], optional
        The file path where the logged messages are stored, by default None.
        If ``None``, then no logging ``FileHandler`` will be used and the logging
        messages are only pushed to stdout. Otherwise, a logging ``FileHandler`` will
        write the logged messages to the given file path.

    Returns
    -------
    Union[List[pd.Series], pd.DataFrame]
        The preprocessed series.

    Notes
    -----
    * If a ``logging_file_path`` is provided, the execution (time) info can be
      retrieved by calling ``logger.get_processor_logs(logging_file_path)``. &lt;br&gt;
      Be aware that the ``logging_file_path`` gets cleared before the logger pushes
      logged messages. Hence, one should use a separate logging file for each
      constructed processing and feature instance with this library.
    * If a series processor its function output is a ``np.ndarray``, the input series
      dict (required dict for that function) must contain just 1 series! That series
      its name and index are used to return a series dict. When a user does not want
      a numpy array to replace its input series, it is his / her responsibility to
      create a new ``pd.Series`` (or ``pd.DataFrame``) of that numpy array with a
      different (column) name.
    * If ``func_output`` is a ``pd.Series``, keep in mind that the input series gets
      transformed (i.e., replaced) in the pipeline with the ``func_output`` when the
      series name is  equal.

    Raises
    ------
    _ProcessingError
        Error raised when a processing step fails.

    &#34;&#34;&#34;
    # Delete other logging handlers
    delete_logging_handlers(logger)
    # Add logging handler (if path provided)
    if logging_file_path:
        add_logging_handler(logger, logging_file_path)

    # Convert the data to a series_dict
    series_dict: Dict[str, pd.Series] = {}
    for s in to_series_list(data):
        # Assert the assumptions we make!
        if len(s):
            assert isinstance(s.index, pd.DatetimeIndex)
        # TODO: also check monotonic increasing?

        if s.name in self.get_required_series():
            series_dict[str(s.name)] = s.copy() if copy else s
        elif return_all_series:
            # If all the series have to be returned
            series_dict[str(s.name)] = s.copy() if copy else s

    output_keys = set()  # Maintain set of output series
    for processor in self.processing_steps:
        try:
            processed_dict = processor(series_dict)
            output_keys.update(processed_dict.keys())
            series_dict.update(processed_dict)
        except Exception as e:
            raise _ProcessingError(
                &#34;Error while processing function {}:\n {}&#34;.format(
                    processor.name, str(e)
                )
            ) from e

    if not return_all_series:
        # Return just the output series
        output_dict = {key: series_dict[str(key)] for key in output_keys}
        series_dict = output_dict

    if drop_keys is not None:
        # Drop the keys that should not be included in the output
        output_dict = {
            key: series_dict[key]
            for key in set(series_dict.keys()).difference(drop_keys)
        }
        series_dict = output_dict

    if return_df:
        # We merge the series dict into a DataFrame
        return series_dict_to_df(series_dict)
    else:
        return [s for s in series_dict.values()]</code></pre>
</details>
<div class="desc"><p>Execute all <code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code> objects in pipeline sequentially.</p>
<p>Apply all the processing steps on passed Series list or DataFrame and return the
preprocessed Series list or DataFrame.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]</code></dt>
<dd>
<p>Dataframe or Series or list thereof, with all the required data for the
processing steps. </p>
<p><strong>Remark</strong>: each Series / DataFrame must have a <code>pd.DatetimeIndex</code>.
<strong>Remark</strong>: we assume that each name / column is unique.</p>
</dd>
<dt><strong><code>return_df</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether the output needs to be a series list or a DataFrame, by default
False.
If True the output series will be combined to a DataFrame with an outer
merge.</dd>
<dt><strong><code>return_all_series</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether the output needs to return all the series, by default True.
* If True the output will contain all series that were passed to this
method.
* If False the output will contain just the required series (see
<code>get_required_series</code>).</dd>
<dt><strong><code>drop_keys</code></strong> :&ensp;<code>List[str]</code>, optional</dt>
<dd>Which keys should be dropped when returning the output, by default None.</dd>
<dt><strong><code>copy</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether the series in <code>data</code> should be copied, by default False.</dd>
<dt><strong><code>logging_file_path</code></strong> :&ensp;<code>Union[str, Path]</code>, optional</dt>
<dd>The file path where the logged messages are stored, by default None.
If <code>None</code>, then no logging <code>FileHandler</code> will be used and the logging
messages are only pushed to stdout. Otherwise, a logging <code>FileHandler</code> will
write the logged messages to the given file path.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[List[pd.Series], pd.DataFrame]</code></dt>
<dd>The preprocessed series.</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li>If a <code>logging_file_path</code> is provided, the execution (time) info can be
retrieved by calling <code><a title="tsflex.processing.logger.get_processor_logs" href="logger.html#tsflex.processing.logger.get_processor_logs">get_processor_logs()</a>(logging_file_path)</code>. <br>
Be aware that the <code>logging_file_path</code> gets cleared before the logger pushes
logged messages. Hence, one should use a separate logging file for each
constructed processing and feature instance with this library.</li>
<li>If a series processor its function output is a <code>np.ndarray</code>, the input series
dict (required dict for that function) must contain just 1 series! That series
its name and index are used to return a series dict. When a user does not want
a numpy array to replace its input series, it is his / her responsibility to
create a new <code>pd.Series</code> (or <code>pd.DataFrame</code>) of that numpy array with a
different (column) name.</li>
<li>If <code>func_output</code> is a <code>pd.Series</code>, keep in mind that the input series gets
transformed (i.e., replaced) in the pipeline with the <code>func_output</code> when the
series name is
equal.</li>
</ul>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>_ProcessingError</code></dt>
<dd>Error raised when a processing step fails.</dd>
</dl></div>
</dd>
<dt id="tsflex.processing.SeriesPipeline.serialize"><code class="name flex">
<span>def <span class="ident">serialize</span></span>(<span>self, file_path)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def serialize(self, file_path: Union[str, Path]):
    &#34;&#34;&#34;Serialize this ``SeriesPipeline`` instance.

    Notes
    ------
    As we use [Dill](https://github.com/uqfoundation/dill){:target=&#34;_blank&#34;} to
    serialize, we can also serialize (decorator)functions which are defined in the
    local scope, like lambdas.

    Parameters
    ----------
    file_path : Union[str, Path]
        The path where the ``SeriesProcessor`` will be serialized.

    &#34;&#34;&#34;
    with open(file_path, &#34;wb&#34;) as f:
        dill.dump(self, f, recurse=True)</code></pre>
</details>
<div class="desc"><p>Serialize this <code><a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a></code> instance.</p>
<h2 id="notes">Notes</h2>
<p>As we use <a href="https://github.com/uqfoundation/dill" target="_blank">Dill</a> to
serialize, we can also serialize (decorator)functions which are defined in the
local scope, like lambdas.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>Union[str, Path]</code></dt>
<dd>The path where the <code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code> will be serialized.</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</article>
<div class="sidebar_container">
<nav id="sidebar">
<div id="sidebar_content">
<header>
<div style="text-align: left; padding-top: 15px;">
<a class="homelink" rel="home" title="tsflex home" href="/tsflex/">
<img src="https://raw.githubusercontent.com/predict-idlab/tsflex/main/docs/_static/logo.png"
alt="logo should be displayed here" width="95%"></a>
</div>
</header>
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#processing-guide">Processing guide</a><ul>
<li><a href="#working-example">Working example âœ…</a></li>
<li><a href="#getting-started">Getting started ðŸš€</a><ul>
<li><a href="#components">Components</a></li>
<li><a href="#processing-functions">Processing functions</a></li>
</ul>
</li>
<li><a href="#important-notes">Important notes ðŸ“¢</a></li>
<li><a href="#advanced-usage">Advanced usage ðŸ‘€</a><ul>
<li><a href="#versatile-processing-functions">Versatile processing functions</a></li>
<li><a href="#dataframe-decorator">DataFrame decorator</a></li>
<li><a href="#logging">Logging</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tsflex" href="../index.html">tsflex</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">tsflex.processing: API reference</a></h3>
<ul>
<li><code><a title="tsflex.processing.logger" href="logger.html">.logger</a></code></li>
<li><code><a title="tsflex.processing.series_pipeline" href="series_pipeline.html">.series_pipeline</a></code></li>
<li><code><a title="tsflex.processing.series_processor" href="series_processor.html">.series_processor</a></code></li>
<li><code><a title="tsflex.processing.utils" href="utils.html">.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tsflex.processing.dataframe_func" href="#tsflex.processing.dataframe_func">dataframe_func</a></code></li>
<li><code><a title="tsflex.processing.get_processor_logs" href="#tsflex.processing.get_processor_logs">get_processor_logs</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tsflex.processing.SeriesProcessor" href="#tsflex.processing.SeriesProcessor">SeriesProcessor</a></code></h4>
<ul class="">
<li><code><a title="tsflex.processing.SeriesProcessor.get_required_series" href="#tsflex.processing.SeriesProcessor.get_required_series">get_required_series</a></code></li>
<li><code><a title="tsflex.processing.SeriesProcessor.__call__" href="#tsflex.processing.SeriesProcessor.__call__">__call__</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="tsflex.processing.SeriesPipeline" href="#tsflex.processing.SeriesPipeline">SeriesPipeline</a></code></h4>
<ul class="">
<li><code><a title="tsflex.processing.SeriesPipeline.get_required_series" href="#tsflex.processing.SeriesPipeline.get_required_series">get_required_series</a></code></li>
<li><code><a title="tsflex.processing.SeriesPipeline.append" href="#tsflex.processing.SeriesPipeline.append">append</a></code></li>
<li><code><a title="tsflex.processing.SeriesPipeline.insert" href="#tsflex.processing.SeriesPipeline.insert">insert</a></code></li>
<li><code><a title="tsflex.processing.SeriesPipeline.process" href="#tsflex.processing.SeriesPipeline.process">process</a></code></li>
<li><code><a title="tsflex.processing.SeriesPipeline.serialize" href="#tsflex.processing.SeriesPipeline.serialize">serialize</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</nav>
</div>
</main>
<script>
const sidebar = document.querySelector("body > main > div");
const sidebar_nav = document.querySelector("body > main > div > nav");
const sidebar_content = document.getElementById("sidebar_content");
document.getElementById("index_button_button").onclick = function () {
sidebar.classList.toggle('sidebar_small');
sidebar_nav.classList.toggle('hide_content');
sidebar_content.classList.toggle('hide_content');
}
</script>
</body>
</html>