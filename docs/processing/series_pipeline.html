<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>tsflex.processing.series_pipeline API documentation</title>
<meta name="description" content="SeriesPipeline class for time-series data (pre-)processing pipeline." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/foundation.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em;padding-left:1em;padding-right:1em}button{display:none}#sidebar{padding:3px;max-width:20em;overflow:hidden;min-width:19.8em}#sidebar > *:last-child{margin-bottom:1cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;border-top:1px solid #ddd;text-align:right}#footer p{}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f1f3f9;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:0.5em;padding:0px}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;max_width:100%;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.info{background:#edfcf4}.admonition.note,.admonition.important{background:#ebf3ff}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#edfcf4}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#ffddcc}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:850px){.sidebar_container{display:flex;transition:0.75s ease}.sidebar_small{width:0;margin:0;padding:0}.hide_content{display:none}button{display:initial;float:left;position:sticky;border:none;height:5ch;width:5ch;border-radius:50%;box-shadow:0px 1px 4px 1px rgba(0,0,0,.2);top:5%;left:100%;transform:translateX(-50%);cursor:pointer}#sidebar{width:25%;height:100vh;overflow:auto;position:sticky;top:0;transition:0.75s ease}#index_button_img{opacity:0.65}#content{max-width:105ch;padding:2em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1em;padding-right:0.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-212611910-1"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-212611910-1');
</script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="https://media.discordapp.net/attachments/372491075153166338/852906324417445908/icon.png">
</head>
<body>
<main>
<article id="content">
<button id="index_button_button"><img id="index_button_img"
src="https://image.flaticon.com/icons/png/512/56/56763.png"
alt="" width="33" height="25"></button>
<header>
<h1 class="title">Module <code>tsflex.processing.series_pipeline</code></h1>
</header>
<section id="section-intro">
<p>SeriesPipeline class for time-series data (pre-)processing pipeline.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;SeriesPipeline class for time-series data (pre-)processing pipeline.&#34;&#34;&#34;
from __future__ import annotations

__author__ = &#34;Jonas Van Der Donckt, Emiel Deprost, Jeroen Van Der Donckt&#34;

from pathlib import Path
from typing import Dict, List, Optional, Set, Union

import dill
import pandas as pd

from ..utils.data import flatten, series_dict_to_df, to_series_list
from ..utils.logging import add_logging_handler, delete_logging_handlers
from .logger import logger
from .series_processor import SeriesProcessor


class _ProcessingError(Exception):
    pass


class SeriesPipeline:
    &#34;&#34;&#34;Pipeline for applying ``SeriesProcessor`` objects sequentially.

    Parameters
    ----------
    processors : List[Union[SeriesProcessor, SeriesPipeline]], optional
        List of ``SeriesProcessor`` or ``SeriesPipeline`` instances that will be applied
        sequentially to the internal series dict, by default None.
        **The processing steps will be executed in the same order as passed in this
        list.**

    &#34;&#34;&#34;

    def __init__(
        self, processors: Optional[List[Union[SeriesProcessor, SeriesPipeline]]] = None
    ):
        self.processing_steps: List[SeriesProcessor] = []  # TODO: dit private of niet?
        if processors is not None:
            assert isinstance(processors, list)

            self.processing_steps = list(
                flatten(
                    [
                        p.processing_steps if isinstance(p, SeriesPipeline) else [p]
                        for p in processors
                    ]
                )
            )

    def get_required_series(self) -&gt; List[str]:
        &#34;&#34;&#34;Return all required series names for this pipeline.

        Return the list of series names that are required in order to execute all the
        ``SeriesProcessor`` objects of this processing pipeline.

        Returns
        -------
        List[str]
            List of all the required series names.

        &#34;&#34;&#34;
        return list(
            set(flatten(step.get_required_series() for step in self.processing_steps))
        )

    def append(self, processor: Union[SeriesProcessor, SeriesPipeline]) -&gt; None:
        &#34;&#34;&#34;Append a ``SeriesProcessor`` at the end of the pipeline.

        Parameters
        ----------
        processor : Union[SeriesProcessor, SeriesPipeline]
            The ``SeriesProcessor`` or ``SeriesPipeline`` that will be added to the
            end of the pipeline.

        &#34;&#34;&#34;
        if isinstance(processor, SeriesProcessor):
            self.processing_steps.append(processor)
        elif isinstance(processor, SeriesPipeline):
            self.processing_steps.extend(processor.processing_steps)
        else:
            raise TypeError(
                &#34;Can only append SeriesProcessor or SeriesPipeline, &#34;
                + f&#34;not {type(processor)}&#34;
            )

    def insert(
        self, idx: int, processor: Union[SeriesProcessor, SeriesPipeline]
    ) -&gt; None:
        &#34;&#34;&#34;Insert a ``SeriesProcessor`` at the given index in the pipeline.

        Parameters
        ----------
        idx : int
            The index where the given processor should be inserted in the pipeline.
            Index 0 will insert the given processor at the front of the pipeline,
            and index ``len(pipeline)`` is equivalent to appending the processor.
        processor : Union[SeriesProcessor, SeriesPipeline]
            The ``SeriesProcessor`` or ``SeriesPipeline`` that will be inserted.&lt;br&gt;
            .. note::
                If the given processor is a ``SeriesPipeline``, all its processors will
                be inserted sequentially, starting from the given index.

        &#34;&#34;&#34;
        if isinstance(processor, SeriesProcessor):
            self.processing_steps.insert(idx, processor)
        elif isinstance(processor, SeriesPipeline):
            for i, ps in enumerate(processor.processing_steps):
                self.insert(idx + i, ps)
        else:
            raise TypeError(
                &#34;Can only insert a SeriesProcessor or SeriesPipeline, &#34;
                + f&#34;not {type(processor)}&#34;
            )

    def process(
        self,
        data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],
        return_df: Optional[bool] = False,
        return_all_series: Optional[bool] = True,
        drop_keys: Optional[List[str]] = None,
        copy: Optional[bool] = False,
        logging_file_path: Optional[Union[str, Path]] = None,
    ) -&gt; Union[List[pd.Series], pd.DataFrame]:
        &#34;&#34;&#34;Execute all ``SeriesProcessor`` objects in pipeline sequentially.

        Apply all the processing steps on passed Series list or DataFrame and return the
        preprocessed Series list or DataFrame.

        Parameters
        ----------
        data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]
            Dataframe or Series or list thereof, with all the required data for the
            processing steps. \n
            **Remark**: each Series / DataFrame must have a ``pd.DatetimeIndex``.
            **Remark**: we assume that each name / column is unique.
        return_df : bool, optional
            Whether the output needs to be a series list or a DataFrame, by default
            False.
            If True the output series will be combined to a DataFrame with an outer
            merge.
        return_all_series : bool, optional
            Whether the output needs to return all the series, by default True.
            * If True the output will contain all series that were passed to this
            method.
            * If False the output will contain just the required series (see
            ``get_required_series``).
        drop_keys : List[str], optional
            Which keys should be dropped when returning the output, by default None.
        copy : bool, optional
            Whether the series in ``data`` should be copied, by default False.
        logging_file_path : Union[str, Path], optional
            The file path where the logged messages are stored, by default None.
            If ``None``, then no logging ``FileHandler`` will be used and the logging
            messages are only pushed to stdout. Otherwise, a logging ``FileHandler`` will
            write the logged messages to the given file path.

        Returns
        -------
        Union[List[pd.Series], pd.DataFrame]
            The preprocessed series.

        Notes
        -----
        * If a ``logging_file_path`` is provided, the execution (time) info can be
          retrieved by calling ``logger.get_processor_logs(logging_file_path)``. &lt;br&gt;
          Be aware that the ``logging_file_path`` gets cleared before the logger pushes
          logged messages. Hence, one should use a separate logging file for each
          constructed processing and feature instance with this library.
        * If a series processor its function output is a ``np.ndarray``, the input series
          dict (required dict for that function) must contain just 1 series! That series
          its name and index are used to return a series dict. When a user does not want
          a numpy array to replace its input series, it is his / her responsibility to
          create a new ``pd.Series`` (or ``pd.DataFrame``) of that numpy array with a
          different (column) name.
        * If ``func_output`` is a ``pd.Series``, keep in mind that the input series gets
          transformed (i.e., replaced) in the pipeline with the ``func_output`` when the
          series name is  equal.

        Raises
        ------
        _ProcessingError
            Error raised when a processing step fails.

        &#34;&#34;&#34;
        # Delete other logging handlers
        delete_logging_handlers(logger)
        # Add logging handler (if path provided)
        if logging_file_path:
            f_handler = add_logging_handler(logger, logging_file_path)

        # Convert the data to a series_dict
        series_dict: Dict[str, pd.Series] = {}
        for s in to_series_list(data):
            # Assert the assumptions we make!
            if len(s):
                assert isinstance(s.index, pd.DatetimeIndex)
            # TODO: also check monotonic increasing?

            if s.name in self.get_required_series():
                series_dict[str(s.name)] = s.copy() if copy else s
            elif return_all_series:
                # If all the series have to be returned
                series_dict[str(s.name)] = s.copy() if copy else s

        output_keys: Set[str] = set()  # Maintain set of output series
        for processor in self.processing_steps:
            try:
                processed_dict = processor(series_dict)
                output_keys.update(processed_dict.keys())
                series_dict.update(processed_dict)
            except Exception as e:
                # Close the file handler (this avoids PermissionError: [WinError 32])
                if logging_file_path:
                    f_handler.close()
                    logger.removeHandler(f_handler)
                raise _ProcessingError(
                    &#34;Error while processing function {}:\n {}&#34;.format(
                        processor.name, str(e)
                    )
                ) from e

        # Close the file handler (this avoids PermissionError: [WinError 32])
        if logging_file_path:
            f_handler.close()
            logger.removeHandler(f_handler)

        if not return_all_series:
            # Return just the output series
            output_dict = {key: series_dict[str(key)] for key in output_keys}
            series_dict = output_dict

        if drop_keys is not None:
            # Drop the keys that should not be included in the output
            output_dict = {
                key: series_dict[key]
                for key in set(series_dict.keys()).difference(drop_keys)
            }
            series_dict = output_dict

        if return_df:
            # We merge the series dict into a DataFrame
            return series_dict_to_df(series_dict)
        else:
            return [s for s in series_dict.values()]

    def serialize(self, file_path: Union[str, Path]) -&gt; None:
        &#34;&#34;&#34;Serialize this ``SeriesPipeline`` instance.

        Notes
        ------
        As we use [Dill](https://github.com/uqfoundation/dill){:target=&#34;_blank&#34;} to
        serialize, we can also serialize (decorator)functions which are defined in the
        local scope, like lambdas.

        Parameters
        ----------
        file_path : Union[str, Path]
            The path where the ``SeriesProcessor`` will be serialized.

        &#34;&#34;&#34;
        with open(file_path, &#34;wb&#34;) as f:
            dill.dump(self, f, recurse=True)

    def __repr__(self) -&gt; str:
        &#34;&#34;&#34;Return formal representation of object.&#34;&#34;&#34;
        return &#34;[\n&#34; + &#34;&#34;.join([f&#34;\t{str(p)}\n&#34; for p in self.processing_steps]) + &#34;]&#34;

    def __str__(self) -&gt; str:
        &#34;&#34;&#34;Return informal representation of object.&#34;&#34;&#34;
        return self.__repr__()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tsflex.processing.series_pipeline.SeriesPipeline"><code class="flex name class">
<span>class <span class="ident">SeriesPipeline</span></span>
<span>(</span><span>processors=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SeriesPipeline:
    &#34;&#34;&#34;Pipeline for applying ``SeriesProcessor`` objects sequentially.

    Parameters
    ----------
    processors : List[Union[SeriesProcessor, SeriesPipeline]], optional
        List of ``SeriesProcessor`` or ``SeriesPipeline`` instances that will be applied
        sequentially to the internal series dict, by default None.
        **The processing steps will be executed in the same order as passed in this
        list.**

    &#34;&#34;&#34;

    def __init__(
        self, processors: Optional[List[Union[SeriesProcessor, SeriesPipeline]]] = None
    ):
        self.processing_steps: List[SeriesProcessor] = []  # TODO: dit private of niet?
        if processors is not None:
            assert isinstance(processors, list)

            self.processing_steps = list(
                flatten(
                    [
                        p.processing_steps if isinstance(p, SeriesPipeline) else [p]
                        for p in processors
                    ]
                )
            )

    def get_required_series(self) -&gt; List[str]:
        &#34;&#34;&#34;Return all required series names for this pipeline.

        Return the list of series names that are required in order to execute all the
        ``SeriesProcessor`` objects of this processing pipeline.

        Returns
        -------
        List[str]
            List of all the required series names.

        &#34;&#34;&#34;
        return list(
            set(flatten(step.get_required_series() for step in self.processing_steps))
        )

    def append(self, processor: Union[SeriesProcessor, SeriesPipeline]) -&gt; None:
        &#34;&#34;&#34;Append a ``SeriesProcessor`` at the end of the pipeline.

        Parameters
        ----------
        processor : Union[SeriesProcessor, SeriesPipeline]
            The ``SeriesProcessor`` or ``SeriesPipeline`` that will be added to the
            end of the pipeline.

        &#34;&#34;&#34;
        if isinstance(processor, SeriesProcessor):
            self.processing_steps.append(processor)
        elif isinstance(processor, SeriesPipeline):
            self.processing_steps.extend(processor.processing_steps)
        else:
            raise TypeError(
                &#34;Can only append SeriesProcessor or SeriesPipeline, &#34;
                + f&#34;not {type(processor)}&#34;
            )

    def insert(
        self, idx: int, processor: Union[SeriesProcessor, SeriesPipeline]
    ) -&gt; None:
        &#34;&#34;&#34;Insert a ``SeriesProcessor`` at the given index in the pipeline.

        Parameters
        ----------
        idx : int
            The index where the given processor should be inserted in the pipeline.
            Index 0 will insert the given processor at the front of the pipeline,
            and index ``len(pipeline)`` is equivalent to appending the processor.
        processor : Union[SeriesProcessor, SeriesPipeline]
            The ``SeriesProcessor`` or ``SeriesPipeline`` that will be inserted.&lt;br&gt;
            .. note::
                If the given processor is a ``SeriesPipeline``, all its processors will
                be inserted sequentially, starting from the given index.

        &#34;&#34;&#34;
        if isinstance(processor, SeriesProcessor):
            self.processing_steps.insert(idx, processor)
        elif isinstance(processor, SeriesPipeline):
            for i, ps in enumerate(processor.processing_steps):
                self.insert(idx + i, ps)
        else:
            raise TypeError(
                &#34;Can only insert a SeriesProcessor or SeriesPipeline, &#34;
                + f&#34;not {type(processor)}&#34;
            )

    def process(
        self,
        data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],
        return_df: Optional[bool] = False,
        return_all_series: Optional[bool] = True,
        drop_keys: Optional[List[str]] = None,
        copy: Optional[bool] = False,
        logging_file_path: Optional[Union[str, Path]] = None,
    ) -&gt; Union[List[pd.Series], pd.DataFrame]:
        &#34;&#34;&#34;Execute all ``SeriesProcessor`` objects in pipeline sequentially.

        Apply all the processing steps on passed Series list or DataFrame and return the
        preprocessed Series list or DataFrame.

        Parameters
        ----------
        data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]
            Dataframe or Series or list thereof, with all the required data for the
            processing steps. \n
            **Remark**: each Series / DataFrame must have a ``pd.DatetimeIndex``.
            **Remark**: we assume that each name / column is unique.
        return_df : bool, optional
            Whether the output needs to be a series list or a DataFrame, by default
            False.
            If True the output series will be combined to a DataFrame with an outer
            merge.
        return_all_series : bool, optional
            Whether the output needs to return all the series, by default True.
            * If True the output will contain all series that were passed to this
            method.
            * If False the output will contain just the required series (see
            ``get_required_series``).
        drop_keys : List[str], optional
            Which keys should be dropped when returning the output, by default None.
        copy : bool, optional
            Whether the series in ``data`` should be copied, by default False.
        logging_file_path : Union[str, Path], optional
            The file path where the logged messages are stored, by default None.
            If ``None``, then no logging ``FileHandler`` will be used and the logging
            messages are only pushed to stdout. Otherwise, a logging ``FileHandler`` will
            write the logged messages to the given file path.

        Returns
        -------
        Union[List[pd.Series], pd.DataFrame]
            The preprocessed series.

        Notes
        -----
        * If a ``logging_file_path`` is provided, the execution (time) info can be
          retrieved by calling ``logger.get_processor_logs(logging_file_path)``. &lt;br&gt;
          Be aware that the ``logging_file_path`` gets cleared before the logger pushes
          logged messages. Hence, one should use a separate logging file for each
          constructed processing and feature instance with this library.
        * If a series processor its function output is a ``np.ndarray``, the input series
          dict (required dict for that function) must contain just 1 series! That series
          its name and index are used to return a series dict. When a user does not want
          a numpy array to replace its input series, it is his / her responsibility to
          create a new ``pd.Series`` (or ``pd.DataFrame``) of that numpy array with a
          different (column) name.
        * If ``func_output`` is a ``pd.Series``, keep in mind that the input series gets
          transformed (i.e., replaced) in the pipeline with the ``func_output`` when the
          series name is  equal.

        Raises
        ------
        _ProcessingError
            Error raised when a processing step fails.

        &#34;&#34;&#34;
        # Delete other logging handlers
        delete_logging_handlers(logger)
        # Add logging handler (if path provided)
        if logging_file_path:
            f_handler = add_logging_handler(logger, logging_file_path)

        # Convert the data to a series_dict
        series_dict: Dict[str, pd.Series] = {}
        for s in to_series_list(data):
            # Assert the assumptions we make!
            if len(s):
                assert isinstance(s.index, pd.DatetimeIndex)
            # TODO: also check monotonic increasing?

            if s.name in self.get_required_series():
                series_dict[str(s.name)] = s.copy() if copy else s
            elif return_all_series:
                # If all the series have to be returned
                series_dict[str(s.name)] = s.copy() if copy else s

        output_keys: Set[str] = set()  # Maintain set of output series
        for processor in self.processing_steps:
            try:
                processed_dict = processor(series_dict)
                output_keys.update(processed_dict.keys())
                series_dict.update(processed_dict)
            except Exception as e:
                # Close the file handler (this avoids PermissionError: [WinError 32])
                if logging_file_path:
                    f_handler.close()
                    logger.removeHandler(f_handler)
                raise _ProcessingError(
                    &#34;Error while processing function {}:\n {}&#34;.format(
                        processor.name, str(e)
                    )
                ) from e

        # Close the file handler (this avoids PermissionError: [WinError 32])
        if logging_file_path:
            f_handler.close()
            logger.removeHandler(f_handler)

        if not return_all_series:
            # Return just the output series
            output_dict = {key: series_dict[str(key)] for key in output_keys}
            series_dict = output_dict

        if drop_keys is not None:
            # Drop the keys that should not be included in the output
            output_dict = {
                key: series_dict[key]
                for key in set(series_dict.keys()).difference(drop_keys)
            }
            series_dict = output_dict

        if return_df:
            # We merge the series dict into a DataFrame
            return series_dict_to_df(series_dict)
        else:
            return [s for s in series_dict.values()]

    def serialize(self, file_path: Union[str, Path]) -&gt; None:
        &#34;&#34;&#34;Serialize this ``SeriesPipeline`` instance.

        Notes
        ------
        As we use [Dill](https://github.com/uqfoundation/dill){:target=&#34;_blank&#34;} to
        serialize, we can also serialize (decorator)functions which are defined in the
        local scope, like lambdas.

        Parameters
        ----------
        file_path : Union[str, Path]
            The path where the ``SeriesProcessor`` will be serialized.

        &#34;&#34;&#34;
        with open(file_path, &#34;wb&#34;) as f:
            dill.dump(self, f, recurse=True)

    def __repr__(self) -&gt; str:
        &#34;&#34;&#34;Return formal representation of object.&#34;&#34;&#34;
        return &#34;[\n&#34; + &#34;&#34;.join([f&#34;\t{str(p)}\n&#34; for p in self.processing_steps]) + &#34;]&#34;

    def __str__(self) -&gt; str:
        &#34;&#34;&#34;Return informal representation of object.&#34;&#34;&#34;
        return self.__repr__()</code></pre>
</details>
<div class="desc"><p>Pipeline for applying <code>SeriesProcessor</code> objects sequentially.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>processors</code></strong> :&ensp;<code>List[Union[SeriesProcessor, <a title="tsflex.processing.series_pipeline.SeriesPipeline" href="#tsflex.processing.series_pipeline.SeriesPipeline">SeriesPipeline</a>]]</code>, optional</dt>
<dd>List of <code>SeriesProcessor</code> or <code><a title="tsflex.processing.series_pipeline.SeriesPipeline" href="#tsflex.processing.series_pipeline.SeriesPipeline">SeriesPipeline</a></code> instances that will be applied
sequentially to the internal series dict, by default None.
<strong>The processing steps will be executed in the same order as passed in this
list.</strong></dd>
</dl></div>
<h3>Methods</h3>
<dl>
<dt id="tsflex.processing.series_pipeline.SeriesPipeline.get_required_series"><code class="name flex">
<span>def <span class="ident">get_required_series</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_required_series(self) -&gt; List[str]:
    &#34;&#34;&#34;Return all required series names for this pipeline.

    Return the list of series names that are required in order to execute all the
    ``SeriesProcessor`` objects of this processing pipeline.

    Returns
    -------
    List[str]
        List of all the required series names.

    &#34;&#34;&#34;
    return list(
        set(flatten(step.get_required_series() for step in self.processing_steps))
    )</code></pre>
</details>
<div class="desc"><p>Return all required series names for this pipeline.</p>
<p>Return the list of series names that are required in order to execute all the
<code>SeriesProcessor</code> objects of this processing pipeline.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[str]</code></dt>
<dd>List of all the required series names.</dd>
</dl></div>
</dd>
<dt id="tsflex.processing.series_pipeline.SeriesPipeline.append"><code class="name flex">
<span>def <span class="ident">append</span></span>(<span>self, processor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append(self, processor: Union[SeriesProcessor, SeriesPipeline]) -&gt; None:
    &#34;&#34;&#34;Append a ``SeriesProcessor`` at the end of the pipeline.

    Parameters
    ----------
    processor : Union[SeriesProcessor, SeriesPipeline]
        The ``SeriesProcessor`` or ``SeriesPipeline`` that will be added to the
        end of the pipeline.

    &#34;&#34;&#34;
    if isinstance(processor, SeriesProcessor):
        self.processing_steps.append(processor)
    elif isinstance(processor, SeriesPipeline):
        self.processing_steps.extend(processor.processing_steps)
    else:
        raise TypeError(
            &#34;Can only append SeriesProcessor or SeriesPipeline, &#34;
            + f&#34;not {type(processor)}&#34;
        )</code></pre>
</details>
<div class="desc"><p>Append a <code>SeriesProcessor</code> at the end of the pipeline.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>processor</code></strong> :&ensp;<code>Union[SeriesProcessor, <a title="tsflex.processing.series_pipeline.SeriesPipeline" href="#tsflex.processing.series_pipeline.SeriesPipeline">SeriesPipeline</a>]</code></dt>
<dd>The <code>SeriesProcessor</code> or <code><a title="tsflex.processing.series_pipeline.SeriesPipeline" href="#tsflex.processing.series_pipeline.SeriesPipeline">SeriesPipeline</a></code> that will be added to the
end of the pipeline.</dd>
</dl></div>
</dd>
<dt id="tsflex.processing.series_pipeline.SeriesPipeline.insert"><code class="name flex">
<span>def <span class="ident">insert</span></span>(<span>self, idx, processor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def insert(
    self, idx: int, processor: Union[SeriesProcessor, SeriesPipeline]
) -&gt; None:
    &#34;&#34;&#34;Insert a ``SeriesProcessor`` at the given index in the pipeline.

    Parameters
    ----------
    idx : int
        The index where the given processor should be inserted in the pipeline.
        Index 0 will insert the given processor at the front of the pipeline,
        and index ``len(pipeline)`` is equivalent to appending the processor.
    processor : Union[SeriesProcessor, SeriesPipeline]
        The ``SeriesProcessor`` or ``SeriesPipeline`` that will be inserted.&lt;br&gt;
        .. note::
            If the given processor is a ``SeriesPipeline``, all its processors will
            be inserted sequentially, starting from the given index.

    &#34;&#34;&#34;
    if isinstance(processor, SeriesProcessor):
        self.processing_steps.insert(idx, processor)
    elif isinstance(processor, SeriesPipeline):
        for i, ps in enumerate(processor.processing_steps):
            self.insert(idx + i, ps)
    else:
        raise TypeError(
            &#34;Can only insert a SeriesProcessor or SeriesPipeline, &#34;
            + f&#34;not {type(processor)}&#34;
        )</code></pre>
</details>
<div class="desc"><p>Insert a <code>SeriesProcessor</code> at the given index in the pipeline.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index where the given processor should be inserted in the pipeline.
Index 0 will insert the given processor at the front of the pipeline,
and index <code>len(pipeline)</code> is equivalent to appending the processor.</dd>
<dt><strong><code>processor</code></strong> :&ensp;<code>Union[SeriesProcessor, <a title="tsflex.processing.series_pipeline.SeriesPipeline" href="#tsflex.processing.series_pipeline.SeriesPipeline">SeriesPipeline</a>]</code></dt>
<dd>The <code>SeriesProcessor</code> or <code><a title="tsflex.processing.series_pipeline.SeriesPipeline" href="#tsflex.processing.series_pipeline.SeriesPipeline">SeriesPipeline</a></code> that will be inserted.<br><div class="admonition note">
<p class="admonition-title">Note</p>
If the given processor is a <code><a title="tsflex.processing.series_pipeline.SeriesPipeline" href="#tsflex.processing.series_pipeline.SeriesPipeline">SeriesPipeline</a></code>, all its processors will
be inserted sequentially, starting from the given index.</div>
</dd>
</dl></div>
</dd>
<dt id="tsflex.processing.series_pipeline.SeriesPipeline.process"><code class="name flex">
<span>def <span class="ident">process</span></span>(<span>self, data, return_df=False, return_all_series=True, drop_keys=None, copy=False, logging_file_path=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process(
    self,
    data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],
    return_df: Optional[bool] = False,
    return_all_series: Optional[bool] = True,
    drop_keys: Optional[List[str]] = None,
    copy: Optional[bool] = False,
    logging_file_path: Optional[Union[str, Path]] = None,
) -&gt; Union[List[pd.Series], pd.DataFrame]:
    &#34;&#34;&#34;Execute all ``SeriesProcessor`` objects in pipeline sequentially.

    Apply all the processing steps on passed Series list or DataFrame and return the
    preprocessed Series list or DataFrame.

    Parameters
    ----------
    data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]
        Dataframe or Series or list thereof, with all the required data for the
        processing steps. \n
        **Remark**: each Series / DataFrame must have a ``pd.DatetimeIndex``.
        **Remark**: we assume that each name / column is unique.
    return_df : bool, optional
        Whether the output needs to be a series list or a DataFrame, by default
        False.
        If True the output series will be combined to a DataFrame with an outer
        merge.
    return_all_series : bool, optional
        Whether the output needs to return all the series, by default True.
        * If True the output will contain all series that were passed to this
        method.
        * If False the output will contain just the required series (see
        ``get_required_series``).
    drop_keys : List[str], optional
        Which keys should be dropped when returning the output, by default None.
    copy : bool, optional
        Whether the series in ``data`` should be copied, by default False.
    logging_file_path : Union[str, Path], optional
        The file path where the logged messages are stored, by default None.
        If ``None``, then no logging ``FileHandler`` will be used and the logging
        messages are only pushed to stdout. Otherwise, a logging ``FileHandler`` will
        write the logged messages to the given file path.

    Returns
    -------
    Union[List[pd.Series], pd.DataFrame]
        The preprocessed series.

    Notes
    -----
    * If a ``logging_file_path`` is provided, the execution (time) info can be
      retrieved by calling ``logger.get_processor_logs(logging_file_path)``. &lt;br&gt;
      Be aware that the ``logging_file_path`` gets cleared before the logger pushes
      logged messages. Hence, one should use a separate logging file for each
      constructed processing and feature instance with this library.
    * If a series processor its function output is a ``np.ndarray``, the input series
      dict (required dict for that function) must contain just 1 series! That series
      its name and index are used to return a series dict. When a user does not want
      a numpy array to replace its input series, it is his / her responsibility to
      create a new ``pd.Series`` (or ``pd.DataFrame``) of that numpy array with a
      different (column) name.
    * If ``func_output`` is a ``pd.Series``, keep in mind that the input series gets
      transformed (i.e., replaced) in the pipeline with the ``func_output`` when the
      series name is  equal.

    Raises
    ------
    _ProcessingError
        Error raised when a processing step fails.

    &#34;&#34;&#34;
    # Delete other logging handlers
    delete_logging_handlers(logger)
    # Add logging handler (if path provided)
    if logging_file_path:
        f_handler = add_logging_handler(logger, logging_file_path)

    # Convert the data to a series_dict
    series_dict: Dict[str, pd.Series] = {}
    for s in to_series_list(data):
        # Assert the assumptions we make!
        if len(s):
            assert isinstance(s.index, pd.DatetimeIndex)
        # TODO: also check monotonic increasing?

        if s.name in self.get_required_series():
            series_dict[str(s.name)] = s.copy() if copy else s
        elif return_all_series:
            # If all the series have to be returned
            series_dict[str(s.name)] = s.copy() if copy else s

    output_keys: Set[str] = set()  # Maintain set of output series
    for processor in self.processing_steps:
        try:
            processed_dict = processor(series_dict)
            output_keys.update(processed_dict.keys())
            series_dict.update(processed_dict)
        except Exception as e:
            # Close the file handler (this avoids PermissionError: [WinError 32])
            if logging_file_path:
                f_handler.close()
                logger.removeHandler(f_handler)
            raise _ProcessingError(
                &#34;Error while processing function {}:\n {}&#34;.format(
                    processor.name, str(e)
                )
            ) from e

    # Close the file handler (this avoids PermissionError: [WinError 32])
    if logging_file_path:
        f_handler.close()
        logger.removeHandler(f_handler)

    if not return_all_series:
        # Return just the output series
        output_dict = {key: series_dict[str(key)] for key in output_keys}
        series_dict = output_dict

    if drop_keys is not None:
        # Drop the keys that should not be included in the output
        output_dict = {
            key: series_dict[key]
            for key in set(series_dict.keys()).difference(drop_keys)
        }
        series_dict = output_dict

    if return_df:
        # We merge the series dict into a DataFrame
        return series_dict_to_df(series_dict)
    else:
        return [s for s in series_dict.values()]</code></pre>
</details>
<div class="desc"><p>Execute all <code>SeriesProcessor</code> objects in pipeline sequentially.</p>
<p>Apply all the processing steps on passed Series list or DataFrame and return the
preprocessed Series list or DataFrame.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]</code></dt>
<dd>
<p>Dataframe or Series or list thereof, with all the required data for the
processing steps. </p>
<p><strong>Remark</strong>: each Series / DataFrame must have a <code>pd.DatetimeIndex</code>.
<strong>Remark</strong>: we assume that each name / column is unique.</p>
</dd>
<dt><strong><code>return_df</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether the output needs to be a series list or a DataFrame, by default
False.
If True the output series will be combined to a DataFrame with an outer
merge.</dd>
<dt><strong><code>return_all_series</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether the output needs to return all the series, by default True.
* If True the output will contain all series that were passed to this
method.
* If False the output will contain just the required series (see
<code>get_required_series</code>).</dd>
<dt><strong><code>drop_keys</code></strong> :&ensp;<code>List[str]</code>, optional</dt>
<dd>Which keys should be dropped when returning the output, by default None.</dd>
<dt><strong><code>copy</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether the series in <code>data</code> should be copied, by default False.</dd>
<dt><strong><code>logging_file_path</code></strong> :&ensp;<code>Union[str, Path]</code>, optional</dt>
<dd>The file path where the logged messages are stored, by default None.
If <code>None</code>, then no logging <code>FileHandler</code> will be used and the logging
messages are only pushed to stdout. Otherwise, a logging <code>FileHandler</code> will
write the logged messages to the given file path.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[List[pd.Series], pd.DataFrame]</code></dt>
<dd>The preprocessed series.</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li>If a <code>logging_file_path</code> is provided, the execution (time) info can be
retrieved by calling <code>logger.get_processor_logs(logging_file_path)</code>. <br>
Be aware that the <code>logging_file_path</code> gets cleared before the logger pushes
logged messages. Hence, one should use a separate logging file for each
constructed processing and feature instance with this library.</li>
<li>If a series processor its function output is a <code>np.ndarray</code>, the input series
dict (required dict for that function) must contain just 1 series! That series
its name and index are used to return a series dict. When a user does not want
a numpy array to replace its input series, it is his / her responsibility to
create a new <code>pd.Series</code> (or <code>pd.DataFrame</code>) of that numpy array with a
different (column) name.</li>
<li>If <code>func_output</code> is a <code>pd.Series</code>, keep in mind that the input series gets
transformed (i.e., replaced) in the pipeline with the <code>func_output</code> when the
series name is
equal.</li>
</ul>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>_ProcessingError</code></dt>
<dd>Error raised when a processing step fails.</dd>
</dl></div>
</dd>
<dt id="tsflex.processing.series_pipeline.SeriesPipeline.serialize"><code class="name flex">
<span>def <span class="ident">serialize</span></span>(<span>self, file_path)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def serialize(self, file_path: Union[str, Path]) -&gt; None:
    &#34;&#34;&#34;Serialize this ``SeriesPipeline`` instance.

    Notes
    ------
    As we use [Dill](https://github.com/uqfoundation/dill){:target=&#34;_blank&#34;} to
    serialize, we can also serialize (decorator)functions which are defined in the
    local scope, like lambdas.

    Parameters
    ----------
    file_path : Union[str, Path]
        The path where the ``SeriesProcessor`` will be serialized.

    &#34;&#34;&#34;
    with open(file_path, &#34;wb&#34;) as f:
        dill.dump(self, f, recurse=True)</code></pre>
</details>
<div class="desc"><p>Serialize this <code><a title="tsflex.processing.series_pipeline.SeriesPipeline" href="#tsflex.processing.series_pipeline.SeriesPipeline">SeriesPipeline</a></code> instance.</p>
<h2 id="notes">Notes</h2>
<p>As we use <a href="https://github.com/uqfoundation/dill" target="_blank">Dill</a> to
serialize, we can also serialize (decorator)functions which are defined in the
local scope, like lambdas.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>Union[str, Path]</code></dt>
<dd>The path where the <code>SeriesProcessor</code> will be serialized.</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</article>
<div class="sidebar_container">
<nav id="sidebar">
<div id="sidebar_content">
<header>
<div style="text-align: left; padding-top: 15px;">
<a class="homelink" rel="home" title="tsflex home" href="/tsflex/">
<img src="https://raw.githubusercontent.com/predict-idlab/tsflex/main/docs/_static/logo.png"
alt="logo should be displayed here" width="95%"></a>
</div>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tsflex.processing" href="index.html">.processing</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tsflex.processing.series_pipeline.SeriesPipeline" href="#tsflex.processing.series_pipeline.SeriesPipeline">SeriesPipeline</a></code></h4>
<ul class="">
<li><code><a title="tsflex.processing.series_pipeline.SeriesPipeline.get_required_series" href="#tsflex.processing.series_pipeline.SeriesPipeline.get_required_series">get_required_series</a></code></li>
<li><code><a title="tsflex.processing.series_pipeline.SeriesPipeline.append" href="#tsflex.processing.series_pipeline.SeriesPipeline.append">append</a></code></li>
<li><code><a title="tsflex.processing.series_pipeline.SeriesPipeline.insert" href="#tsflex.processing.series_pipeline.SeriesPipeline.insert">insert</a></code></li>
<li><code><a title="tsflex.processing.series_pipeline.SeriesPipeline.process" href="#tsflex.processing.series_pipeline.SeriesPipeline.process">process</a></code></li>
<li><code><a title="tsflex.processing.series_pipeline.SeriesPipeline.serialize" href="#tsflex.processing.series_pipeline.SeriesPipeline.serialize">serialize</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</nav>
</div>
</main>
<script>
const sidebar = document.querySelector("body > main > div");
const sidebar_nav = document.querySelector("body > main > div > nav");
const sidebar_content = document.getElementById("sidebar_content");
document.getElementById("index_button_button").onclick = function () {
sidebar.classList.toggle('sidebar_small');
sidebar_nav.classList.toggle('hide_content');
sidebar_content.classList.toggle('hide_content');
}
</script>
</body>
</html>